{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a80850f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef test():\\n    net = ResNet50()\\n    x = torch.randn(2, 1, 100, 100)  # 4 dim, 2 pictures with 3 channels af 224 pixels in each. \\n    y = net(x)\\n    print(y.size())\\n\\n#test()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the network\n",
    "import torch.nn as nn\n",
    "\n",
    "class baseblock(nn.Module):\n",
    "    expansion = 1\n",
    "    basic = True\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(baseblock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample # identity_downsample = convlayer, which we might need if we change the input sizes or number of channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class block(nn.Module):\n",
    "    expansion = 4 # Number of blocks after a channel is always 4 times higher than when it entered; ref paper\n",
    "    basic = False\n",
    "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
    "        super(block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) # normalize the batches, such that our output data don't variate too much \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels,out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample # identity_downsample = convlayer, which we might need if we change the input sizes or number of channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        x = self.conv1(x)        \n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class ResNet(nn.Module): # [3,4,6,3]: how many times the blocks are used in each layer (4 layers)\n",
    "    def __init__(self, block, layers, image_channels, num_classes): # image_channels= 3(RGB), 1(MNIST) etc. num_classes = how many classes we want to find(3,6,8 MNIST pictures) \n",
    "        super(ResNet, self).__init__()\n",
    "        # Initialize modules\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(in_channels=image_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False) # initial layer, haven't done anything of yet\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, layers[0], out_channels=64, stride=1)\n",
    "        self.layer2 = self._make_layer(block, layers[1], out_channels=128, stride=2)\n",
    "        self.layer3 = self._make_layer(block, layers[2], out_channels=256, stride=2)\n",
    "        self.layer4 = self._make_layer(block, layers[3], out_channels=512, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512* block.expansion, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, out_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "        if block.basic:\n",
    "            stride =1\n",
    "\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            identity_downsample = nn.Sequential(nn.Conv2d(in_channels=self.in_channels, out_channels=out_channels*block.expansion,\n",
    "                                                            kernel_size=1, stride=stride, bias=False),nn.BatchNorm2d(out_channels*block.expansion))\n",
    "        \n",
    "        layers.append(block(self.in_channels, out_channels, identity_downsample, stride))\n",
    "        \n",
    "        self.in_channels = out_channels * block.expansion\n",
    "\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "def ResNet50(img_channels = 1, num_classes = 10):\n",
    "    return ResNet(block, [3, 4, 6, 3], img_channels, num_classes)\n",
    "\n",
    "def ResNet18(img_channels = 1, num_classes = 10):\n",
    "    return ResNet(baseblock, [2,2,2,2], img_channels, num_classes)\n",
    "    \n",
    "def ResNetX(img_channels = 1, num_classes = 10, layers = [2,2,2,2]):\n",
    "    return ResNet(baseblock, layers, img_channels, num_classes )\n",
    "\n",
    "'''\n",
    "\n",
    "def test():\n",
    "    net = ResNet50()\n",
    "    x = torch.randn(2, 1, 100, 100)  # 4 dim, 2 pictures with 3 channels af 224 pixels in each. \n",
    "    y = net(x)\n",
    "    print(y.size())\n",
    "\n",
    "#test()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760d3a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GPU.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./temp/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./temp/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./temp/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./temp/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "16.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./temp/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./temp/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./temp/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./temp/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./temp/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./temp/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 50000, Validation size: 50000:60000, batch sizes: 400,neural_networs: 16, Epochs_ResNet: 7, Epochs_SuperLoop: 16\n",
      "Epoch  1 : Train Loss 0.085612 , Train acc 0.958660, Valid acc 0.958200\n"
     ]
    }
   ],
   "source": [
    "# loading packages\n",
    "from operator import index\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n",
    "\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import accuracy_score\n",
    "import ResNet as RN\n",
    "#import sys\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")\n",
    "\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n",
    "def TrainNN(layers): # Layers: [x,x,x,x], block: Which block type(base or botteneck)\n",
    "    # Defining the network\n",
    "    net = RN.ResNetX(img_channels = channels, num_classes = classes, layers = layers)\n",
    "    if use_cuda:\n",
    "        net.cuda()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-7) # Stochastic gradient descent\n",
    "    criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
    "\n",
    "    num_samples_train = x_train.shape[0]\n",
    "    num_batches_train = num_samples_train// batch_size\n",
    "    num_samples_valid = x_valid.shape[0]\n",
    "    num_batches_valid = num_samples_valid// batch_size\n",
    "\n",
    "    # Setting up the lists for handling loss/accurazy\n",
    "    train_acc, train_loss = [],[]\n",
    "    valid_acc, valid_loss = [],[]\n",
    "    test_acc, test_loss = [],[]\n",
    "\n",
    "    cur_loss = 0\n",
    "    losses = []\n",
    "\n",
    "\n",
    "    get_slice  = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        # Forward -> Backprob -> update params\n",
    "        ## Train\n",
    "\n",
    "        cur_loss = 0\n",
    "        net.train() # Telling putorch we are training the network now\n",
    "        for i in range(num_batches_train):\n",
    "            optimizer.zero_grad() # Setting all the gradients to zero, probably shound't be necessary\n",
    "            slce = get_slice(i, batch_size) # Using our Lambda function to calculate the slices\n",
    "            #print(x_train[slce].size())\n",
    "            # Wrapping in Variables\n",
    "            input = Variable(get_variable(x_train[slce]))\n",
    "            output = net(input) # I think only training the batch we are looking at \n",
    "            #output = output.reshape(len(output)) # reshaping\n",
    "            # Compute gradients given loss\n",
    "            target_batch = Variable(get_variable(targets_train[slce])) # Finding the targets for the current batch/slice\n",
    "            #print(output,output.shape)\n",
    "            #print(target_batch, target_batch.shape)\n",
    "            batch_loss = criterion(output, target_batch) # Calculating the losses based on the current batch\n",
    "            batch_loss.backward() # finding all of the loss gradients\n",
    "            optimizer.step() # optimizing the gradients, taking the next step\n",
    "\n",
    "            cur_loss += get_numpy(batch_loss) #Adding the loss for this batch\n",
    "        \n",
    "        losses.append(cur_loss/batch_size) # Append the losses\n",
    "        if epoch%5 == 0 or epoch == num_epochs:\n",
    "            ### Evaluating training\n",
    "            net.eval() # Telling pytourch we are evaluating the data now and not training\n",
    "            train_preds, train_targs = [], []\n",
    "            for i in range(num_batches_train):\n",
    "                slce = get_slice(i, batch_size) \n",
    "                input = Variable(get_variable(x_train[slce]))\n",
    "                output = net(input) # Running the training date trough the network\n",
    "\n",
    "                preds = torch.max(output,1)[1] # Finding the maximum value of the output for each batch\n",
    "                train_targs += list(targets_train[slce].numpy()) # Adding the data to list\n",
    "                train_preds += list(get_numpy(preds)) # Not quite sure\n",
    "\n",
    "            ### Evaluate validation\n",
    "            val_preds, val_targs = [],[]\n",
    "            for i in range(num_batches_valid):\n",
    "                slce = get_slice(i, batch_size)\n",
    "                input = Variable(get_variable(x_valid[slce]))\n",
    "                output = net(input)\n",
    "                preds = torch.max(output,1)[1]\n",
    "                val_targs += list(targets_valid[slce].numpy())\n",
    "                val_preds += list(get_numpy(preds))\n",
    "\n",
    "            train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "            valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "            \n",
    "            train_acc.append(train_acc_cur)\n",
    "            valid_acc.append(valid_acc_cur)\n",
    "            #if epoch % 10 == 0:\n",
    "            print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                    epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "        else:\n",
    "            print(\"Epoch %2i : Train Loss %f\" % (epoch+1, losses[-1]))\n",
    "    \n",
    "    return losses[-1] # .detach().numpy() # return the last lost\n",
    " \n",
    "\n",
    "def choose_layers(b, l): # A method for returning how many times each block should be repeated\n",
    "    layers = [] # The number of layers for each block\n",
    "    index = [] # The index corrosponding to each layer(used later when updating probs)\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        numbers = [x for x in range(len(b[i]))]\n",
    "        index.append(np.random.choice(numbers,p=b[i]))\n",
    "        layers.append(l[i][index[i]])\n",
    "    return layers, index # Returning two list of length b_i = 4\n",
    "\n",
    "def choose_optimal_layers(b,l):\n",
    "    layers = []\n",
    "    for i in range(len(b)):\n",
    "        layers.append(l[i][np.argmax(b[i])])\n",
    "    return layers\n",
    "\n",
    "def zero_mean_rewards(losses):\n",
    "    mean_value = np.mean(losses)\n",
    "    rewards = np.zeros(len(losses))\n",
    "    for i in range(len(losses)):\n",
    "        rewards[i] = (losses[i] - mean_value)/mean_value \n",
    "    return rewards\n",
    "'''\n",
    "def logits(probs):\n",
    "    for i in range(len(probs)):\n",
    "        probs[i] = np.log(probs[i]/(1-probs[i]))\n",
    "    return probs\n",
    "'''\n",
    "def logits(probs):\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] == 1: # To avoid dividing with 0\n",
    "            a = np.ones(len(probs))*(-30)\n",
    "            a[i] = 30\n",
    "            return a\n",
    "        probs[i] = np.log(probs[i]/(1-probs[i]))\n",
    "    return probs\n",
    "\n",
    "def softmax(vec):\n",
    "    max_number = np.max(vec)\n",
    "    vec = vec - max_number\n",
    "    exponential = np.exp(vec)\n",
    "    probabilities = exponential / np.sum(exponential)\n",
    "    '''\n",
    "    for i in range(len(vec)):\n",
    "        if probabilities[i] < 1e-5:\n",
    "            probabilities[i] = 0\n",
    "    '''\n",
    "    return probabilities\n",
    "\n",
    "def update_probs(b_i_old, ResNets_index, rewards, i, alpha=1): # i is the b_i we are looking at \n",
    "\n",
    "    # Finding the logits of b_i\n",
    "    lgt_i_old = logits(b_i_old) \n",
    "\n",
    "    # Finding the softmax values for the logits used in the updating step. Think logits and softmax are cancelling eachother\n",
    "    sft_max = softmax(lgt_i_old)\n",
    "\n",
    "    # Creating a vector for loop values (4)\n",
    "    upd_stp = np.zeros(len(b_i_old)) # It's all initalised at zero\n",
    "    print(lgt_i_old)\n",
    "    print(sft_max)\n",
    "    print(upd_stp)\n",
    "    print(ResNets_index)\n",
    "    # The updating step -> the gradient of our softmax likelihood function\n",
    "    for n in range(len(rewards)):\n",
    "        slct_val = ResNets_index[n][i] # the selected value for network j when looking at b_i\n",
    "\n",
    "        upd_stp[slct_val] -= rewards[n] * (1-sft_max[slct_val]) # the negativ is since we are using the loss as rewards\n",
    "    \n",
    "    # Taking the mean \n",
    "    upd_stp = upd_stp/len(rewards)\n",
    "    \n",
    "    # Multiplying with the alpha value\n",
    "    upd_stp = upd_stp * alpha\n",
    "    \n",
    "    # Finally we can the updating steps to the old logits\n",
    "    lgt_i_new = lgt_i_old + upd_stp\n",
    "\n",
    "    # lastly we take the softmax such that we get the probabilites\n",
    "    b_i_new = softmax(lgt_i_new)\n",
    "\n",
    "    return b_i_new\n",
    "\n",
    "\n",
    "#arg_list = sys.argv\n",
    "# Importing the MNIST dataset\n",
    "mnist_trainset = MNIST(\"./temp/\", train=True, download=True) # Size of 60000\n",
    "mnist_testset = MNIST(\"./temp/\", train=False, download=True) # Size of 60000\n",
    "# Only taking a subset\n",
    "tra_size = 50000 # using 5/6 as training size\n",
    "val_size = 60000 # using 1/6 as validation size\n",
    "\n",
    "x_train = mnist_trainset.data[:tra_size].view(-1, 784).float()\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, 28, 28))\n",
    "targets_train = mnist_trainset.targets[:tra_size]\n",
    "\n",
    "#quit()\n",
    "#targets_train = targets_train.to(torch.float)\n",
    "\n",
    "x_valid = mnist_trainset.data[tra_size:val_size].view(-1, 784).float()\n",
    "x_valid = x_valid.reshape((x_valid.shape[0], 1, 28, 28))\n",
    "targets_valid = mnist_trainset.targets[tra_size:val_size]\n",
    "#targets_valid = targets_valid.to(torch.float)\n",
    "\n",
    "# Building the training loop\n",
    "# Normalizing the inputs\n",
    "x_train.div_(255)\n",
    "x_valid.div_(255)\n",
    "\n",
    "# Defining the loss function and the optimizer\n",
    "channels = 1 # b/w  = 1 channel\n",
    "classes = 10 # Numbers to predict\n",
    "batch_size = 400\n",
    "num_epochs = 7\n",
    "\n",
    "\n",
    "# Initialising super parameters\n",
    "super_loop = 16 # 10-100\n",
    "n_networks = 16\n",
    "b = np.array([[1/3,1/3,1/3], [1/3,1/3,1/3], [1/3,1/3,1/3], [1/3,1/3,1/3]])\n",
    "l = np.array([[1,2,3], [1,2,3], [1,2,3], [1,2,3]])\n",
    "\n",
    "b_print = np.empty([super_loop,len(b),len(b[0])])\n",
    "avg_loss = [] # the average loss for each iteration size [super_loop] * [n_networks]\n",
    "# Training the super archicture \n",
    "\n",
    "# Printing the parameters: \n",
    "print(f'Training size: {tra_size}, Validation size: {tra_size}:{val_size}, batch sizes: {batch_size},neural_networs: {n_networks}, Epochs_ResNet: {num_epochs}, Epochs_SuperLoop: {super_loop}')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for k in range(super_loop):\n",
    "    b_print[k] = b\n",
    "    #print(f'Epoch {k} : b_values {b} ')\n",
    "    losses = np.zeros(n_networks) # array for the losses\n",
    "\n",
    "    # 'creating' n ResNets\n",
    "    ResNets_layers = np.empty([n_networks,len(l)], dtype=int)\n",
    "    ResNets_index = np.empty([n_networks,len(l)], dtype=int)\n",
    "\n",
    "    for i in range(n_networks):\n",
    "        # ResNets_layers and ResNets_index is n*len(b) (10*4)\n",
    "        layers, index = choose_layers(b,l)\n",
    "        ResNets_layers[i] = layers\n",
    "        ResNets_index[i] = index\n",
    "\n",
    "    # Training the networks\n",
    "    for i in range(n_networks):\n",
    "        ll = TrainNN(layers=ResNets_layers[i]) \n",
    "        losses[i] = ll # losses is an list of n elements (10)\n",
    "    avg_loss.append(losses) # For printing\n",
    "\n",
    "    # Calculating the zero mean ranking rewards \n",
    "    rewards = zero_mean_rewards(losses) # rewards is a list of n elements (10) \n",
    "\n",
    "    #Updating the probabilities\n",
    "    for i in range(len(b)): # Updating the b-values one probability at a time (4)\n",
    "        b[i] = update_probs(b[i], ResNets_index, rewards, i)\n",
    "\n",
    "\n",
    "# Finding optimal variables\n",
    "opt_layers = choose_optimal_layers(b,l)\n",
    "\n",
    "# Training the optimal network\n",
    "loss = TrainNN(layers=opt_layers)\n",
    "\n",
    "#\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f'The final loss when training with the optimal layers is: {loss}')\n",
    "\n",
    "# Printing the optimized probabilites\n",
    "print('b_print[i]')\n",
    "for i in range(super_loop):\n",
    "    print(b_print[i])\n",
    "\n",
    "np.savetxt(f'NASNet_tsize_{tra_size}_vsize_{val_size}_Bsize_{batch_size}_nnetworks_{n_networks}_Epoch_ResNet_{num_epochs}_Epoch_Super_{super_loop}.txt',avg_loss)\n",
    "x = [i+1 for i in range(super_loop)]\n",
    "for xe, ye in zip(x,avg_loss):\n",
    "    plt.scatter([xe] * len(ye), ye, label=f'{xe}')\n",
    "plt.title('Results over Superloop')\n",
    "plt.xlabel('Iterations in the SuperLoop')\n",
    "plt.ylabel('Training error')\n",
    "plt.legend()\n",
    "plt.savefig(f'NASNet_tsize_{tra_size}_vsize_{val_size}_Bsize_{batch_size}_nnetworks_{n_networks}_Epoch_ResNet_{num_epochs}_Epoch_Super_{super_loop}.png')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41df59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
